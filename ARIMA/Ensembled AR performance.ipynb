{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from AutoRegression import AutoRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA, ARMA\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from ARIMA import myARIMA\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(satellite_predicted_values, satellite_true_values): \n",
    "    # the division, addition and subtraction are pointwise \n",
    "    return np.mean(np.abs((satellite_predicted_values - satellite_true_values) \n",
    "        / (np.abs(satellite_predicted_values) + np.abs(satellite_true_values))))\n",
    "\n",
    "\n",
    "def delete_duplicates(df, eps=10):\n",
    "    \"\"\"\n",
    "        Returns df without \"duplicates\" - objects within each sat_id,\n",
    "        which were recorded at almost same time\n",
    "    \"\"\"\n",
    "    \n",
    "    for sat_id in df['sat_id'].unique():\n",
    "        d_t = df[df['sat_id'] == sat_id].epoch.apply(lambda x: time.mktime(datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%f').timetuple())).values\n",
    "        df = df.drop(index=df[df['sat_id'] == sat_id].index[0] + np.where((np.roll(d_t, -1)[:-1] - d_t[:-1]) < eps)[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import smape_idao, idao_score, smape_new_vector_norm\n",
    "\n",
    "def print_metrics(model_name, pred):\n",
    "    print(f\"\\n{model_name}\\n\")\n",
    "    print(f\"IDAO score:  {idao_score(pred[target_columns], df_test_ans[target_columns])}\")    \n",
    "    print(f\"SMAPE IDAO:  {smape_idao(pred[target_columns], df_test_ans[target_columns])}\")    \n",
    "    print(f\"SMAPE new:   {smape_new_vector_norm(pred, df_test_ans)}\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDAO DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\", index_col=\"id\")\n",
    "df_test = pd.read_csv(\"data/test.csv\", index_col=\"id\")\n",
    "df_test_ans = pd.read_csv(\"data/ans.csv\", index_col=\"id\")\n",
    "df_test_ans['sat_id'] = df_test['sat_id']\n",
    "\n",
    "n_sat = len(pd.unique(df_train[\"sat_id\"]))\n",
    "\n",
    "df_train = delete_duplicates(df_train)\n",
    "full_test_wout_dup = delete_duplicates(df_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894a1048a630467daeb1d0d72bf5f93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x     0\n",
      "y     0\n",
      "z     0\n",
      "Vx    0\n",
      "Vy    0\n",
      "Vz    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "width_list = [48,  24]  ## widths of window, which will be used for prediction\n",
    "coefs_for_ensemble = [0.8, 0.2] ## coefs for ensemblin models\n",
    "\n",
    "\n",
    "# predict for test dataset without duplicates:\n",
    "target_columns = ['x', 'y', 'z', 'Vx', 'Vy', 'Vz']\n",
    "for col in target_columns:\n",
    "    full_test_wout_dup[col] = 0.0\n",
    "\n",
    "for sat_id in tqdm(full_test_wout_dup.sat_id.unique()):\n",
    "    for width, coef in zip(width_list, coefs_for_ensemble):\n",
    "        df = df_train[df_train.sat_id == sat_id]\n",
    "        test_satid_mask = full_test_wout_dup.sat_id == sat_id\n",
    "        for col in target_columns:\n",
    "            model = AutoRegression(width=width)\n",
    "            model.fit(df[col].values)\n",
    "            full_test_wout_dup.loc[full_test_wout_dup[test_satid_mask].index, col] +=\\\n",
    "                coef * model.predict(len(full_test_wout_dup[test_satid_mask]))\n",
    "\n",
    "\n",
    "## after this we need to predict for deleted objects (duplicates) from test,\n",
    " ## so we decided to fill predicts with values of nearest objects in time domain\n",
    "final_df = pd.concat([df_test, full_test_wout_dup[target_columns]], axis=1)\n",
    "final_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# for column in target_columns:\n",
    "#     final_df[column] += full_test[column + \"_sim\"].values\n",
    "## saving predictions\n",
    "#final_df[target_columns].to_csv('testing1.csv', index_label='id')\n",
    "## check to prevent submit errors\n",
    "print(final_df[target_columns].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensembled AR\n",
      "\n",
      "IDAO score:  97.3468792352761\n",
      "SMAPE IDAO:  0.02653120764723891\n",
      "SMAPE new:   0.05270116348768325\n"
     ]
    }
   ],
   "source": [
    "print_metrics('Ensembled AR', final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensembled AR\n",
      "\n",
      "IDAO score:  97.21670572063574\n",
      "SMAPE IDAO:  0.02783294279364254\n",
      "SMAPE new:   0.056036577373006685\n"
     ]
    }
   ],
   "source": [
    "print_metrics('Ensembled AR', final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LPC_RP DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train_LPC_RP.csv\", index_col=\"id\")\n",
    "df_test = pd.read_csv(\"data/test_LPC_RP.csv\", index_col=\"id\")\n",
    "df_test_ans = pd.read_csv(\"data/ans_LPC_RP.csv\", index_col=\"id\")\n",
    "df_test_ans['sat_id'] = df_test['sat_id']\n",
    "\n",
    "n_sat = len(pd.unique(df_train[\"sat_id\"]))\n",
    "\n",
    "df_train = delete_duplicates(df_train)\n",
    "full_test_wout_dup = delete_duplicates(df_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592665a7515b4ddab3b28b806c45a930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=225), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x     0\n",
      "y     0\n",
      "z     0\n",
      "Vx    0\n",
      "Vy    0\n",
      "Vz    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "width_list = [48,  24]  ## widths of window, which will be used for prediction\n",
    "coefs_for_ensemble = [0.8, 0.2] ## coefs for ensemblin models\n",
    "\n",
    "\n",
    "# predict for test dataset without duplicates:\n",
    "target_columns = ['x', 'y', 'z', 'Vx', 'Vy', 'Vz']\n",
    "for col in target_columns:\n",
    "    full_test_wout_dup[col] = 0.0\n",
    "\n",
    "for sat_id in tqdm(full_test_wout_dup.sat_id.unique()):\n",
    "    for width, coef in zip(width_list, coefs_for_ensemble):\n",
    "        df = df_train[df_train.sat_id == sat_id]\n",
    "        test_satid_mask = full_test_wout_dup.sat_id == sat_id\n",
    "        for col in target_columns:\n",
    "            model = AutoRegression(width=width)\n",
    "            model.fit(df[col].values)\n",
    "            full_test_wout_dup.loc[full_test_wout_dup[test_satid_mask].index, col] +=\\\n",
    "                coef * model.predict(len(full_test_wout_dup[test_satid_mask]))\n",
    "\n",
    "\n",
    "## after this we need to predict for deleted objects (duplicates) from test,\n",
    " ## so we decided to fill predicts with values of nearest objects in time domain\n",
    "final_df = pd.concat([df_test, full_test_wout_dup[target_columns]], axis=1)\n",
    "final_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# for column in target_columns:\n",
    "#     final_df[column] += full_test[column + \"_sim\"].values\n",
    "## saving predictions\n",
    "#final_df[target_columns].to_csv('testing1.csv', index_label='id')\n",
    "## check to prevent submit errors\n",
    "print(final_df[target_columns].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensembled AR\n",
      "\n",
      "IDAO score:  84.33080112870671\n",
      "SMAPE IDAO:  0.15669198871293286\n",
      "SMAPE new:   0.11743396965482611\n"
     ]
    }
   ],
   "source": [
    "print_metrics('Ensembled AR', final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
